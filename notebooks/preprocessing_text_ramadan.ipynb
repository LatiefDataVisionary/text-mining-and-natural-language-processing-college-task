{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnhYQxKMacDmkuOpdCT4Im",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/text-mining-and-natural-language-processing-college-task/blob/main/preprocessing_text_ramadan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSlBZGJSTcBc",
        "outputId": "be8785bc-41bc-49a0-a3d3-c202e0259225"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf9IZXbkTBf5",
        "outputId": "7e347aa6-f9a4-4c84-aeee-1b1041e60705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contoh Hasil Preprocessing:\n",
            "Original Text: Ramadan Mubarak to all Muslim! https://t.co/D9QoB5eyjd\n",
            "Cleaned Text: Ramadan Mubarak to all Muslim\n",
            "Case Folded: ramadan mubarak to all muslim\n",
            "Tokens: ['ramadan', 'mubarak', 'to', 'all', 'muslim']\n",
            "Filtered Tokens: ['ramadan', 'mubarak', 'muslim']\n",
            "Stemmed Tokens: ['ramadan', 'mubarak', 'muslim']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Download necessary NLTK data packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "\n",
        "# Baca data\n",
        "df = pd.read_csv('fix_combined_ramadan.csv')\n",
        "\n",
        "# 1. Cleaning\n",
        "def clean_text(text):\n",
        "    # Handle NaN\n",
        "    text = str(text) if not pd.isna(text) else ''\n",
        "\n",
        "    # Remove URL\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove mention dan hashtag\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "    # Remove karakter khusus dan angka\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['full_text'].apply(clean_text)\n",
        "\n",
        "# 2. Case Folding\n",
        "df['case_folded'] = df['cleaned_text'].str.lower()\n",
        "\n",
        "# 3. Tokenizing\n",
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "df['tokens'] = df['case_folded'].apply(tokenize_text)\n",
        "\n",
        "# 4. Filtering (Stopword Removal)\n",
        "stop_words = set(stopwords.words('indonesian') + stopwords.words('english') + ['rt'])\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n",
        "\n",
        "# 5. Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_text(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "df['stemmed_tokens'] = df['filtered_tokens'].apply(stem_text)\n",
        "\n",
        "# Simpan hasil preprocessing\n",
        "df.to_csv('preprocessed_ramadan.csv', index=False)\n",
        "\n",
        "# Contoh hasil preprocessing\n",
        "print(\"Contoh Hasil Preprocessing:\")\n",
        "print(\"Original Text:\", df['full_text'][0])\n",
        "print(\"Cleaned Text:\", df['cleaned_text'][0])\n",
        "print(\"Case Folded:\", df['case_folded'][0])\n",
        "print(\"Tokens:\", df['tokens'][0])\n",
        "print(\"Filtered Tokens:\", df['filtered_tokens'][0])\n",
        "print(\"Stemmed Tokens:\", df['stemmed_tokens'][0])"
      ]
    }
  ]
}
